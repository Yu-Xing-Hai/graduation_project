# dhc 项目目录说明（可直接复制到 README.md 中）

该项目是**强直性脊柱炎合并炎症性肠病（IBD）医疗场景大模型 RAG 系统**，聚焦于利用 Qwen-7B 大模型结合检索增强生成（RAG）技术，提供精准、有依据的医疗诊疗建议。以下是详细目录结构及文件注释：

```Plain Text

dhc/  # 项目根目录
```

## 一、数据目录 `data/`

存放项目所有数据（原始数据、处理后数据、RAG 知识库数据），遵循「原始数据-处理数据-业务专用数据」的分层规范，方便数据管理和后续复现。

|子目录/文件|核心作用|当前状态|后续用途|
|---|---|---|---|
|`processed/`|存放经过清洗、格式化、标注后的处理数据|空目录|1. 后续 RAG 数据预处理结果存放<br>2. LoRA 微调标注数据存放<br>3. 数据格式转换（如 pdf→txt）结果存放|
|`rag/`|RAG 系统核心知识库目录，存放用于检索增强的医疗文本数据|空目录|1. 放入临床指南、论文摘要、脱敏病例等 txt 格式文档<br>2. 自动生成 Chroma 向量数据库文件<br>3. 作为 RAG 检索的数据源，支撑模型生成有依据的回答|
|`raw/`|存放未经任何处理的原始数据，保持数据原貌|空目录|1. 存放下载的原始医疗指南、论文原文、公开数据集<br>2. 不修改原始数据，确保数据可追溯<br>3. 后续预处理的数据源基础|
## 二、模型目录 `model/`

存放模型相关所有资源（本地模型权重、微调 checkpoint、运行脚本），是项目的核心功能目录，已完成 Qwen-7B 本地模型的部署和缓存。

### 1. 微调 checkpoint 目录 `lora_checkpoints/`

```Plain Text

model/lora_checkpoints/
```

- 核心作用：存放后续 LoRA 轻量化微调过程中生成的训练权重、日志、配置文件等 checkpoint 数据。

- 当前状态：空目录，预留到位。

- 后续用途：LoRA 微调时，每轮训练的权重会自动保存到该目录，支持断点续训和模型效果回溯。

### 2. 本地模型权重目录 `qwen-7b-local/`

```Plain Text

model/qwen-7b-local/
```

- 核心作用：存放 Qwen-7B 大模型的完整本地权重文件和配置文件，脱离网络依赖，支持快速加载和推理。

- 包含文件及注释：

|    文件|    核心作用|
|---|---|
|    `config.json`|    模型核心配置文件，定义模型结构、维度、训练参数等基础信息|
|    `configuration_qwen.py`|    Qwen 模型专属配置类文件，支撑模型初始化和参数解析|
|    `cpp_kernels.py`|    Qwen 模型加速相关 cpp 内核封装文件，优化模型运行效率|
|    `generation_config.json`|    模型生成配置文件，定义默认生成长度、温度、top_p 等推理参数|
|    `model-00001~00008-of-00008.safetensors`|    Qwen-7B 模型权重分片文件（共 8 个），存储模型的核心参数权重，总大小约 13GB|
|    `modeling_qwen.py`|    Qwen 模型核心结构实现文件，定义模型的网络层、注意力机制等核心逻辑|
|    `model.safetensors.index.json`|    权重分片索引文件，管理 8 个权重分片的对应关系，支撑模型分片加载|
|    `qwen_generation_utils.py`|    Qwen 模型生成相关工具类文件，提供自定义生成逻辑、解码策略等|
|    `qwen.tiktoken`|    Qwen 模型专属分词器词典文件，支撑中文文本的分词和编码|
|    `tokenization_qwen.py`|    Qwen 模型分词器实现文件，负责文本→token 序列的转换和反向转换|
|    `tokenizer_config.json`|    分词器配置文件，定义分词器类型、最大长度、特殊符号等参数|
- 当前状态：文件完整，均为独立实际文件（无软链接），可直接加载使用。

- 后续用途：作为 RAG 系统的核心推理模型，支撑检索增强后的回答生成，无需额外下载网络资源。

### 3. 运行脚本目录 `scripts/`

```Plain Text

model/scripts/
```

- 核心作用：存放模型下载、推理、RAG 构建的所有脚本文件，是项目功能的执行入口。

- 包含文件及注释：

|    文件/目录|    核心作用|
|---|---|
|    `hf_cache/`|    Hugging Face 模型缓存目录，存放 Qwen-7B 模型下载过程中的原始缓存数据（含 blobs、snapshots 子目录）|
|    `hf_cache/models--Qwen--Qwen-7B/`|    具体模型缓存目录，blobs 存放哈希命名的原始数据，snapshots 存放模型快照软链接|
|    `qwen_download_test.py`|    Qwen-7B 模型核心测试脚本，实现模型下载（已完成）、本地加载、医疗场景推理测试，验证模型可用性|
- 当前状态：`qwen_download_test.py` 已验证通过，可正常输出医疗场景回答；`hf_cache` 为冗余缓存目录（`qwen-7b-local/` 已独立），可选择删除释放 13GB 空间。

- 后续用途：新增 `rag_demo.py` 等 RAG 相关脚本，存放所有自定义运行逻辑，保持脚本统一管理。

## 三、项目说明文档 `README.md`

```Plain Text

dhc/README.md
```

- 核心作用：项目总说明文档，记录项目背景、目录结构、环境配置、运行步骤、关键结果、后续规划等内容。

- 当前状态：待完善，已包含本目录注释。

- 后续用途：作为毕设答辩、项目复现、团队协作的核心参考文档，逐步补充详细操作指南和结果分析。

## 补充说明（毕设后续维护参考）

1. 目录规范：所有新增文件/目录遵循「功能分类、命名清晰」原则，避免冗余和混乱，方便后续答辩展示和复现。

2. 冗余文件：`model/scripts/hf_cache/` 目录已无依赖，验证 `qwen-7b-local/` 可用后可删除，释放存储空间。

3. 核心流程：后续将以 `data/rag/` 为数据源，以 `model/qwen-7b-local/` 为推理模型，在 `model/scripts/` 中构建 RAG 系统，实现「文本检索→增强生成」的医疗问答功能。
> （注：文档部分内容可能由 AI 生成）